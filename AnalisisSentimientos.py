# -*- coding: utf-8 -*-
"""AnalisisSentimientos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12KrcGdtRje7roMks9meJ8g6cPVCZKCEo
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install vadersentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

import pandas as pd

# Lee el archivo Parquet y carga los datos en un DataFrame
df = pd.read_parquet('/content/drive/MyDrive/Proyecto_Final/hoteles_review_estado_actualizado.parquet')

# Imprime el DataFrame

df.head(2)

df.info()

#se filtran las columna comentario
new_df = df[['comentario']].copy()
new_df.rename(columns={'comentario':'review'}, inplace=True)

new_df.head()

def limpiar_tokenizar(texto):
    '''
    Esta función limpia y tokeniza el texto en palabras individuales.
    El orden en el que se va limpiando el texto no es arbitrario.
    El listado de signos de puntuación se ha obtenido de: print(string.punctuation)
    y re.escape(string.punctuation)
    '''
    if isinstance(texto, float):  # Verificar si el valor es de tipo float
        return []  # Devolver una lista vacía en caso de que sea float

    # Se convierte todo el texto a minúsculas
    nuevo_texto = texto.lower()
    # Se convierte todo el texto a minúsculas
    nuevo_texto = texto.lower()
    # Eliminación de páginas web (palabras que empiezan por "http")
    nuevo_texto = re.sub('http\S+', ' ', nuevo_texto)
    # Eliminación de signos de puntuación
    regex = '[\\!\\"\\#\\$\\%\\&\\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^_\\`\\{\\|\\}\\~]'
    nuevo_texto = re.sub(regex , ' ', nuevo_texto)
    # Eliminación de números
    nuevo_texto = re.sub("\d+", ' ', nuevo_texto)
    # Eliminación de espacios en blanco múltiples
    nuevo_texto = re.sub("\\s+", ' ', nuevo_texto)
    # Tokenización por palabras individuales
    nuevo_texto = nuevo_texto.split(sep = ' ')
    # Eliminación de tokens con una longitud < 2
    nuevo_texto = [token for token in nuevo_texto if len(token) > 1]

    return(nuevo_texto)

import re

# Se aplica la función de limpieza y tokenización a cada pelicula
# ==============================================================================
new_df['review_tokenizado'] = new_df['review'].apply(lambda x: limpiar_tokenizar(x))
new_df[['review', 'review_tokenizado']].head()

# Unnest de la columna comentario_tokenizado
# ==============================================================================
review_tidy = new_df.explode(column='review_tokenizado')
review_tidy  = review_tidy .drop(columns='review')
review_tidy  = review_tidy .rename(columns={'review_tokenizado':'token'})
review_tidy .head(3)

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
# Obtención de listado de stopwords del inglés
# ==============================================================================
stop_words = list(stopwords.words('english'))
# Se añade la stoprword: amp, ax, ex
stop_words.extend(("amp", "xa", "xe"))
print(stop_words[:10])

# Filtrado para excluir stopwords
# ==============================================================================
review_tidy= review_tidy[~(review_tidy["token"].isin(stop_words))]

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Unir todos los tokens en un solo texto
all_tokens = ' '.join(review_tidy['token'].astype(str))

# Crear el objeto WordCloud
wordcloud = WordCloud(width=800, height=400, background_color='black').generate(all_tokens)

# Mostrar la nube de palabras
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""# Análisis de Sentimientos Utilizando el Modelo Vader"""

import pandas as pd
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Descargar el recurso necesario para VADER (ejecuta solo una vez)
nltk.download('vader_lexicon')

# Inicializar el analizador de sentimientos de VADER
analyzer = SentimentIntensityAnalyzer()

# Función para obtener el sentimiento
def get_sentiment_score(review):
    return analyzer.polarity_scores(review)['compound']

# Calcular el sentimiento para cada revisión y agregarlo como columna 'sentiment'
new_df['sentiment'] = new_df['review'].apply(get_sentiment_score)

new_df.head()

# Función para obtener la etiqueta 'feel' basada en el sentimiento
def get_feel(sentiment):
    if sentiment >= 0.5:
        return 'positive'
    elif -0.5 < sentiment < 0.5:
        return 'neutral'
    else:
        return 'negative'

# Agregar la columna 'feel' con las etiquetas basadas en el sentimiento
new_df['feel'] = new_df['sentiment'].apply(get_feel)

new_df.head()

# Obtener la tabla de frecuencias absolutas
frecuencia_absoluta = new_df['feel'].value_counts()

# Obtener la tabla de frecuencias porcentuales
frecuencia_porcentual = new_df['feel'].value_counts(normalize=True) * 100

# Crear un DataFrame con las tablas de frecuencias
tabla_frecuencias = pd.DataFrame({'Frecuencia Absoluta': frecuencia_absoluta, 'Frecuencia Porcentual': frecuencia_porcentual})

tabla_frecuencias